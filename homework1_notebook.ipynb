{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Artificial Intelligence\n",
    "## Stefano Brilli s249914 - homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Analysis definition\n",
    "In this homework, I will use the tecnique called Principal Components Analysis (PCA) to reduce the dimensionality of the dataset provided. In particular, we want to study how PCA works on images compression.\n",
    "\n",
    "#### 1.1 PCA briefly explanation\n",
    "Principal Components Analysis (PCA) is a tecnique that allows us to summarize a large data set with a smaller number of representative variables that collectively explain most of the variability of out original set.\n",
    "It is useful when we have a large data set, in which each observation has many features (or properties). In this scenario, it is difficult to represent data onto just 2 dimensional space and so itâ€™s difficult to observe how data are correlated among them.\n",
    "The tecnique uses the eigenvectors and eigenvalues of the correlation matrix related to our set to reproject the observations in a new lower dimensional space. The aim of this method is to reduce the dimensionality by keeping as more information as possible.\n",
    "\n",
    "#### 1.2 Tools used\n",
    "The programming language used is Python (version 3.7).\n",
    "The main libraries used to perform these analysis are NumPy for using arrays (http://www.numpy.org/), Sklearn for using machine learning tecniques (http://scikit-learn.org/stable/documentation.html), MatPlotLib for plotting images (https://matplotlib.org/) and Pillow to handle images (PIL, https://pillow.readthedocs.io/en/3.1.x/index.html). So, the first thing to do is to import the above libraries in order to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colo\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Data preparation\n",
    "The dataset contains 1087 samples with 3x227x227 sample size. The goal is to build a matrix NxM, where each row represents an image and each columns represent a feature. Since each image has three dimensions, I have to flat them to obtain a vector representing the pixels and it can be done by using the method ravel() of numpy library. The following function loads in memory the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method opens each class folder and gets raw pixels of each image\n",
    "def getData(directory_name, x, label, y):\n",
    "    directory = os.fsencode(directory_name)\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        i = Image.open(directory_name + filename)\n",
    "        x.extend(np.asarray(i))\n",
    "        global count\n",
    "        y.insert(count, label)\n",
    "        count += 1\n",
    "        global numbers\n",
    "        numbers[label]  = numbers[label] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function getData(...) I have filled some vectors such as x, y and numbers.\n",
    "The latter one is used to count how many images there are for each class label. As we will see soon, the dataset isn't divided perfectly in four subsets according to the class label. This fact will bring some consequences to the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector representing how many elements there are in each folder\n",
    "# 0:dog 1:guitar 2:house 3:person\n",
    "numbers = [0, 0, 0, 0]\n",
    "x = [] # list of items\n",
    "y = [] # list of labels\n",
    "count = 0\n",
    "X_t = [] # here I save the eigenvectors of my dataset according to the number of PC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it's possible to load the images in memory, before starting with the next part. To do that, I have declared the path of the dataset and called four times the function stated before.\n",
    "After loading images, I've used the reshape() method to flat each image as a matrix's row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with subfolders\n",
    "rootFolder = '/home/stefano/Documenti/Politecnico/Magistrale/2 Anno/ML/Homework/#1/PACS_homework/' # root images folder\n",
    "folder1 = 'dog/'\n",
    "folder2 = 'guitar/'\n",
    "folder3 = 'house/'\n",
    "folder4 = 'person/'\n",
    "\n",
    "getData(rootFolder + folder1, x, 0, y) # subset of dog images\n",
    "getData(rootFolder + folder2, x, 1, y) # subset of guitar images\n",
    "getData(rootFolder + folder3, x, 2, y) # subset of house images\n",
    "getData(rootFolder + folder4, x, 3, y) # subset of person images\n",
    "\n",
    "x = np.asarray(x, dtype=np.float64) # all 3D images\n",
    "x_r = np.reshape(x, (1087,154587)) # vectorial representation of matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Principal Components Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceding, I've to perform a stardardization on data. This is useful in particular for classification, but also for plotting data in a simple reference system. The standardization consists of subtracting the dataset's mean from any sample, and dividing each one by the standard deviation. Any feature of the resulting dataset will be then in the form 0 <= xi <= 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To turn back to original values distribution I've to keep the scaler object in a variable\n",
    "scaler = StandardScaler()\n",
    "x_r = scaler.fit_transform(x_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
